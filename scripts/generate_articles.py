#!/usr/bin/env python3
"""Generate articles from raw RSS items using Claude Haiku."""

import json
import re
from collections import defaultdict
from datetime import datetime, timezone
from pathlib import Path

import yaml
from anthropic import Anthropic

from config import (
    RSS_RAW_DIR, CONTENT_DIR, HAIKU_MODEL,
    MAX_ARTICLES_PER_RUN, ARTICLE_MIN_WORDS, ARTICLE_MAX_WORDS,
    SYNTHESIS_MIN_WORDS, SYNTHESIS_MAX_WORDS,
)

SYSTEM_PROMPT = """You are an AI financial news reporter for the Molt Street Journal. Write concise, factual news articles.

Rules:
- Neutral, factual tone — no speculation or opinion
- Include a "## Key Takeaways" section with bullet points
- End with a brief forward-looking statement (next meeting, earnings date, etc.) when relevant
- Do not fabricate facts — only use information from the provided source material
- Do not include a headline (it will be added separately)
- End with: "---\\n*This article was generated by an AI reporter based on the sources listed above.*"
"""

SYNTHESIS_PROMPT = """You are an AI financial news reporter for the Molt Street Journal. Write comprehensive synthesis articles that weave together multiple sources on the same topic.

Rules:
- Neutral, factual tone — no speculation or opinion
- Acknowledge where sources agree or differ in their reporting
- Provide broader context for the story
- Include a "## Key Takeaways" section with 4-5 bullet points
- End with a brief forward-looking statement when relevant
- Do not fabricate facts — only use information from the provided source material
- Do not include a headline (it will be added separately)
- End with: "---\\n*This article was generated by an AI reporter based on the sources listed above.*"
"""

# Known financial entities for extraction
_KNOWN_ENTITIES = {
    "S&P 500", "Dow Jones", "Nasdaq", "Russell 2000", "FTSE 100", "Nikkei 225",
    "Federal Reserve", "Fed", "ECB", "Bank of England", "Bank of Japan",
    "SEC", "CFTC", "FDIC", "Treasury", "IMF", "World Bank",
    "Wall Street", "Silicon Valley",
}

_TICKER_RE = re.compile(r'\b[A-Z]{1,5}\b')
_MULTI_WORD_RE = re.compile(r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)\b')

# Valid enum values for metadata
_VALID_CONTENT_TYPES = {"brief", "synthesis"}
_VALID_SENTIMENTS = {"neutral", "bullish", "bearish"}
_VALID_IMPACTS = {"low", "medium", "high"}


def slugify(text: str) -> str:
    """Convert text to URL-safe slug."""
    text = text.lower().strip()
    text = re.sub(r"[^\w\s-]", "", text)
    text = re.sub(r"[\s_]+", "-", text)
    return text[:80].strip("-")


def extract_title_entities(title: str) -> set[str]:
    """Extract entities from a title using regex heuristics."""
    entities = set()

    # Check known entities
    for entity in _KNOWN_ENTITIES:
        if entity.lower() in title.lower():
            entities.add(entity)

    # Multi-word capitalized phrases (e.g. "Jerome Powell", "Goldman Sachs")
    for match in _MULTI_WORD_RE.finditer(title):
        phrase = match.group(1)
        # Skip common non-entity phrases
        if phrase.split()[0].lower() not in {"the", "a", "an", "in", "on", "at", "for", "to", "and", "or"}:
            entities.add(phrase)

    return entities


def group_items(items: list[dict]) -> list[list[dict]]:
    """Group related RSS items by shared entities. Groups of 2-5 become synthesis; singletons are briefs."""
    # Extract entities for each item
    item_entities = []
    for item in items:
        entities = extract_title_entities(item["title"])
        item_entities.append(entities)

    # Build entity -> item indices mapping
    entity_to_items = defaultdict(set)
    for idx, entities in enumerate(item_entities):
        for entity in entities:
            entity_to_items[entity].add(idx)

    # Greedy grouping: assign items to groups via shared entities
    assigned = set()
    groups = []

    for entity, indices in sorted(entity_to_items.items(), key=lambda x: -len(x[1])):
        unassigned = indices - assigned
        if len(unassigned) >= 2:
            group_indices = sorted(unassigned)[:5]  # cap at 5
            groups.append([items[i] for i in group_indices])
            assigned.update(group_indices)

    # Remaining singletons
    for idx, item in enumerate(items):
        if idx not in assigned:
            groups.append([item])

    return groups


def generate_article(client: Anthropic, items: list[dict]) -> dict | None:
    """Call Claude Haiku to generate an article from RSS items."""
    is_synthesis = len(items) >= 2
    source_text = "\n\n".join(
        f"Source: {item['title']}\nURL: {item['link']}\nSummary: {item['summary']}"
        for item in items
    )

    if is_synthesis:
        word_range = f"{SYNTHESIS_MIN_WORDS}-{SYNTHESIS_MAX_WORDS}"
        system = SYNTHESIS_PROMPT
        max_tokens = 2048
    else:
        word_range = f"{ARTICLE_MIN_WORDS}-{ARTICLE_MAX_WORDS}"
        system = SYSTEM_PROMPT
        max_tokens = 1024

    prompt = f"""Write a {word_range} word news article based on these sources:

{source_text}

Also provide structured metadata. Format your response exactly as:

HEADLINE: <headline>
SUMMARY: <one-sentence summary>
TAGS: <comma-separated tags>
CONTENT_TYPE: {"synthesis" if is_synthesis else "brief"}
ENTITIES: <entity1 (type), entity2 (type), ...>
SENTIMENT: <neutral|bullish|bearish>
IMPACT: <low|medium|high>
SUBCATEGORY: <specific subcategory like monetary-policy, earnings, crypto-regulation, etc.>

<article body>"""

    try:
        response = client.messages.create(
            model=HAIKU_MODEL,
            max_tokens=max_tokens,
            system=system,
            messages=[{"role": "user", "content": prompt}],
        )
    except Exception as e:
        print(f"  API error: {e}")
        return None

    text = response.content[0].text.strip()

    # Parse structured fields from response
    lines = text.split("\n")
    headline = ""
    summary = ""
    tags = []
    content_type = "synthesis" if is_synthesis else "brief"
    entities = []
    sentiment = "neutral"
    impact = "low"
    subcategory = ""
    body_start = 0

    for i, line in enumerate(lines):
        if line.startswith("HEADLINE:"):
            headline = line[9:].strip().strip('"')
        elif line.startswith("SUMMARY:"):
            summary = line[8:].strip()
        elif line.startswith("TAGS:"):
            tags = [t.strip().lower() for t in line[5:].split(",") if t.strip()]
        elif line.startswith("CONTENT_TYPE:"):
            val = line[13:].strip().lower()
            if val in _VALID_CONTENT_TYPES:
                content_type = val
        elif line.startswith("ENTITIES:"):
            raw_entities = line[9:].strip()
            for ent in raw_entities.split(","):
                ent = ent.strip()
                # Parse "Federal Reserve (org)" format
                match = re.match(r"(.+?)\s*\((\w+)\)", ent)
                if match:
                    entities.append({"name": match.group(1).strip(), "type": match.group(2).strip()})
                elif ent:
                    entities.append({"name": ent, "type": "unknown"})
        elif line.startswith("SENTIMENT:"):
            val = line[10:].strip().lower()
            if val in _VALID_SENTIMENTS:
                sentiment = val
        elif line.startswith("IMPACT:"):
            val = line[7:].strip().lower()
            if val in _VALID_IMPACTS:
                impact = val
        elif line.startswith("SUBCATEGORY:"):
            subcategory = line[12:].strip().lower()
            body_start = i + 1
            break

    # Fallback body-start detection: find first line that doesn't look like metadata
    if body_start == 0:
        metadata_prefixes = ("HEADLINE:", "SUMMARY:", "TAGS:", "CONTENT_TYPE:", "ENTITIES:", "SENTIMENT:", "IMPACT:", "SUBCATEGORY:")
        for i, line in enumerate(lines):
            stripped = line.strip()
            if stripped and not any(stripped.startswith(p) for p in metadata_prefixes):
                body_start = i
                break

    body = "\n".join(lines[body_start:]).strip()

    if not headline or not body:
        print("  Failed to parse response")
        return None

    now = datetime.now(timezone.utc)
    slug = slugify(headline)

    return {
        "title": headline,
        "slug": slug,
        "date": now,
        "summary": summary,
        "tags": tags,
        "category": items[0].get("category", "general"),
        "content_type": content_type,
        "entities": entities,
        "sentiment": sentiment,
        "impact": impact,
        "subcategory": subcategory,
        "sources": [{"url": item["link"], "title": item["title"], "feed": item["feed"]} for item in items],
        "body": body,
    }


def save_article(article: dict):
    """Save article as markdown with YAML frontmatter."""
    date = article["date"]
    date_dir = CONTENT_DIR / f"{date.year}" / f"{date.month:02d}" / f"{date.day:02d}"
    date_dir.mkdir(parents=True, exist_ok=True)

    path = date_dir / f"{article['slug']}.md"

    frontmatter = {
        "title": article["title"],
        "slug": article["slug"],
        "date": article["date"].isoformat(),
        "sources": article["sources"],
        "tags": article["tags"],
        "category": article["category"],
        "content_type": article["content_type"],
        "entities": article["entities"],
        "sentiment": article["sentiment"],
        "impact": article["impact"],
        "subcategory": article["subcategory"],
        "reporter": "claude-haiku",
        "summary": article["summary"],
    }

    content = "---\n" + yaml.dump(frontmatter, default_flow_style=False, allow_unicode=True) + "---\n\n" + article["body"] + "\n"
    path.write_text(content, encoding="utf-8")
    print(f"  Saved: {path}")
    return path


def get_unprocessed_items() -> list[dict]:
    """Load all raw RSS items that haven't been turned into articles yet."""
    items = []
    for json_file in sorted(RSS_RAW_DIR.glob("batch_*.json")):
        batch = json.loads(json_file.read_text())
        items.extend(batch)
    return items[:MAX_ARTICLES_PER_RUN]


def run():
    """Generate articles from raw RSS items."""
    items = get_unprocessed_items()
    if not items:
        print("No raw items to process.")
        return

    print(f"Processing {len(items)} items...")
    client = Anthropic()
    groups = group_items(items)

    generated = 0
    for group in groups:
        label = f"{group[0]['title'][:60]}..."
        if len(group) >= 2:
            label = f"[SYNTHESIS {len(group)} sources] {label}"
        print(f"\nGenerating from: {label}")
        article = generate_article(client, group)
        if article:
            save_article(article)
            generated += 1

    print(f"\nGenerated {generated} articles from {len(items)} items.")

    # Clear processed raw files
    for json_file in RSS_RAW_DIR.glob("batch_*.json"):
        json_file.unlink()
    print("Cleared raw RSS data.")


if __name__ == "__main__":
    run()
