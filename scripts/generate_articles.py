#!/usr/bin/env python3
"""Generate articles from raw RSS items using Claude Haiku."""

import json
import re
from datetime import datetime, timezone
from pathlib import Path

import yaml
from anthropic import Anthropic

from config import (
    RSS_RAW_DIR, CONTENT_DIR, HAIKU_MODEL,
    MAX_ARTICLES_PER_RUN, ARTICLE_MIN_WORDS, ARTICLE_MAX_WORDS,
)

SYSTEM_PROMPT = f"""You are an AI financial news reporter for the Molt Street Journal. Write concise, factual news articles.

Rules:
- Write {ARTICLE_MIN_WORDS}-{ARTICLE_MAX_WORDS} words
- Neutral, factual tone — no speculation or opinion
- Include a "## Key Takeaways" section with 3-4 bullet points
- End with a brief forward-looking statement (next meeting, earnings date, etc.) when relevant
- Do not fabricate facts — only use information from the provided source material
- Do not include a headline (it will be added separately)
- End with: "---\\n*This article was generated by an AI reporter based on the sources listed above.*"
"""


def slugify(text: str) -> str:
    """Convert text to URL-safe slug."""
    text = text.lower().strip()
    text = re.sub(r"[^\w\s-]", "", text)
    text = re.sub(r"[\s_]+", "-", text)
    return text[:80].strip("-")


def group_items(items: list[dict]) -> list[list[dict]]:
    """Group related RSS items. For now, each item becomes one article."""
    # Future: cluster by similarity
    return [[item] for item in items]


def generate_article(client: Anthropic, items: list[dict]) -> dict | None:
    """Call Claude Haiku to generate an article from RSS items."""
    source_text = "\n\n".join(
        f"Source: {item['title']}\nURL: {item['link']}\nSummary: {item['summary']}"
        for item in items
    )

    prompt = f"""Write a news article based on these sources:

{source_text}

Also provide a headline and a one-sentence summary. Format your response exactly as:

HEADLINE: <headline>
SUMMARY: <one-sentence summary>
TAGS: <comma-separated tags>

<article body>"""

    try:
        response = client.messages.create(
            model=HAIKU_MODEL,
            max_tokens=1024,
            system=SYSTEM_PROMPT,
            messages=[{"role": "user", "content": prompt}],
        )
    except Exception as e:
        print(f"  API error: {e}")
        return None

    text = response.content[0].text.strip()

    # Parse structured fields from response
    lines = text.split("\n")
    headline = ""
    summary = ""
    tags = []
    body_start = 0

    for i, line in enumerate(lines):
        if line.startswith("HEADLINE:"):
            headline = line[9:].strip().strip('"')
        elif line.startswith("SUMMARY:"):
            summary = line[8:].strip()
        elif line.startswith("TAGS:"):
            tags = [t.strip().lower() for t in line[5:].split(",")]
            body_start = i + 1
            break

    body = "\n".join(lines[body_start:]).strip()

    if not headline or not body:
        print("  Failed to parse response")
        return None

    now = datetime.now(timezone.utc)
    slug = slugify(headline)

    return {
        "title": headline,
        "slug": slug,
        "date": now,
        "summary": summary,
        "tags": tags,
        "category": items[0].get("category", "general"),
        "sources": [{"url": item["link"], "title": item["title"], "feed": item["feed"]} for item in items],
        "body": body,
    }


def save_article(article: dict):
    """Save article as markdown with YAML frontmatter."""
    date = article["date"]
    date_dir = CONTENT_DIR / f"{date.year}" / f"{date.month:02d}" / f"{date.day:02d}"
    date_dir.mkdir(parents=True, exist_ok=True)

    path = date_dir / f"{article['slug']}.md"

    frontmatter = {
        "title": article["title"],
        "slug": article["slug"],
        "date": article["date"].isoformat(),
        "sources": article["sources"],
        "tags": article["tags"],
        "category": article["category"],
        "reporter": "claude-haiku",
        "summary": article["summary"],
    }

    content = "---\n" + yaml.dump(frontmatter, default_flow_style=False, allow_unicode=True) + "---\n\n" + article["body"] + "\n"
    path.write_text(content, encoding="utf-8")
    print(f"  Saved: {path}")
    return path


def get_unprocessed_items() -> list[dict]:
    """Load all raw RSS items that haven't been turned into articles yet."""
    items = []
    for json_file in sorted(RSS_RAW_DIR.glob("batch_*.json")):
        batch = json.loads(json_file.read_text())
        items.extend(batch)
    return items[:MAX_ARTICLES_PER_RUN]


def run():
    """Generate articles from raw RSS items."""
    items = get_unprocessed_items()
    if not items:
        print("No raw items to process.")
        return

    print(f"Processing {len(items)} items...")
    client = Anthropic()
    groups = group_items(items)

    generated = 0
    for group in groups:
        print(f"\nGenerating from: {group[0]['title'][:60]}...")
        article = generate_article(client, group)
        if article:
            save_article(article)
            generated += 1

    print(f"\nGenerated {generated} articles from {len(items)} items.")

    # Clear processed raw files
    for json_file in RSS_RAW_DIR.glob("batch_*.json"):
        json_file.unlink()
    print("Cleared raw RSS data.")


if __name__ == "__main__":
    run()
